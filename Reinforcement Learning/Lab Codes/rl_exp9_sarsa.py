# -*- coding: utf-8 -*-
"""RL exp9 SARSA

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MhmfHMLjxlWGFLUFP0Z9m1IysCUqrK6B
"""

import numpy as np
import gym

# Set the parameters
epsilon = 0.9
total_episodes = 10000
max_steps = 100
alpha = 0.85
gamma = 0.95

# Create the FrozenLake environment
env = gym.make("FrozenLake-v1")

# Initialize the Q-table with zeros
Q = np.zeros([env.observation_space.n, env.action_space.n])

def choose_action(state):
    if np.random.uniform(0, 1) < epsilon:
        action = env.action_space.sample()
    else:
        action = np.argmax(Q[state, :])
    return action

# Implement the SARSA algorithm
rewards = []
for episode in range(total_episodes):
    state = env.reset()
    action = choose_action(state)
    total_reward = 0

    for step in range(max_steps):
        new_state, reward, done, _ = env.step(action)
        new_action = choose_action(new_state)

        # SARSA update rule
        Q[state, action] = Q[state, action] + alpha * (reward + gamma * Q[new_state, new_action] - Q[state, action])

        total_reward += reward
        state = new_state
        action = new_action

        if done:
            break

    rewards.append(total_reward)

# Evaluate the performance
average_reward = sum(rewards) / total_episodes
print("Average reward over {} episodes: {}".format(total_episodes, average_reward))