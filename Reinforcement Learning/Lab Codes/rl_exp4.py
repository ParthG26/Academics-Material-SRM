# -*- coding: utf-8 -*-
"""RL exp4

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JHYchY36_Go8wJfNNI4d-h-Z6BbsX0wg
"""

import pandas as pd
import numpy as np

#import data in the form of csv file
trans_data = pd.read_csv('/content/transitions.csv',header=None)
rd_data = pd.read_csv('/content/rewards.csv',header=None)
rwd_data = pd.read_csv('/content/rewards.csv',header=None)

trans_data = trans_data.to_numpy()
t_data = pd.read_csv('/content/transitions.csv',header=None)
rwd_data = rwd_data.to_numpy()

print(rd_data)

print(trans_data)
print(rwd_data)
gamma = 0.9

#preparation of the input data and storing it into a np matrix
transitions = {}
len_data = trans_data.shape[0]
td = trans_data
for i in range(1,len_data):
  if (td[i][0] in transitions):
    if td[i][1] in transitions[td[i][0]]:
      transitions[td[i][0]][td[i][1]].append((float(td[i][3]),td[i][2]))
    else:
      transitions[td[i][0]][td[i][1]] = [(float(td[i][3]),td[i][2])]
  else:
    transitions[td[i][0]] = {td[i][1]:[(float(td[i][3]),td[i][2])]}
print(trans_data.shape)
for i in transitions.keys():
  print(i)

rewards = {}
rd = rwd_data
len_rewards = rd.shape[0]
for i in range(0,len_rewards):
  rewards[rd[i][0]] = float(rd[i][1]) if rd[i][1] != 'None' else np.nan
print(len(rewards.keys()))

rkeys = rewards.keys()
tkeys = transitions.keys()
st = ''
st2 = ''
for i in tkeys:
  st += i + ' '
for j in rkeys:
  st2 += j + ' '
print(st)
print(st2)

class MarkovDecisionProcess:
    def __init__(self, states=[], transition={}, reward={}, gamma=0.9):
        self.states = states
        self.transition = transition
        self.reward = reward
        self.gamma = gamma

    def Rwd(self, state):
        return self.reward[state]

    def Trans(self, state, action):
        return self.transition[state][action]

    def action(self, state):
        return list(self.transition[state].keys())

def q_value(mdp, s, a, U):
    """Calculate the Q-value for a state-action pair."""
    return sum(p * U[s1] for (p, s1) in mdp.Trans(s, a))

Transitions = transitions
Rewards = rewards
States = transitions.keys()
print(States)

def val_iteration(mdp, epsilon=0.2):
    U1 = {s: 0 for s in mdp.states}
    while True:
        U = U1.copy()
        delta = 0
        for s in mdp.states:
            U1[s] = mdp.Rwd(s) + mdp.gamma * max(q_value(mdp, s, a, U) for a in mdp.action(s))
            delta = max(delta, abs(U1[s] - U[s]))
        if delta <= epsilon * (1 - mdp.gamma) / mdp.gamma:
            return U

# Initialize MDP with your States, Transitions, and Rewards
mdp = MarkovDecisionProcess(states=States, transition=Transitions, reward=Rewards)

# Execute Value Iteration
U = val_iteration(mdp)

# Print the resulting utility values
print(U)

def expected_utility(a,s,V):
  Trans = mdp.Trans
  return sum([p*V[s1] for (p,s1) in Trans(s,a)])

def best_policy(mdp, V):
    pi_policy = {}  # Initialize Policy: a dictionary mapping states to actions
    for s in mdp.states:  # For each state s in the set of states
        best_action = None
        best_value = float('-inf')
        for a in mdp.action(s):  # For each action a available in state s
            utility = sum(p * V[s1] for (p, s1) in mdp.Trans(s, a))  # Calculate expected utility
            if utility > best_value:  # If this action leads to better utility, update best_action
                best_value = utility
                best_action = a
        pi_policy[s] = best_action  # Assign the chosen action to the state s in the policy
    return pi_policy

# Assuming the mdp instance is already created
V = val_iteration(mdp)  # Correctly pass the mdp instance to val_iteration
print("State-Value")
for s in V:
    print(s, ' - ', V[s])

pi = best_policy(mdp, V)  # Correctly pass mdp and V to best_policy
print("\nOptimal policy is\nState - Action")
for s in pi:  # Iterate over states in the policy
    print(s, " - ", pi[s])

